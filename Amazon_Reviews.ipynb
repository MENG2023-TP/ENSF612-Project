{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "    <td>\n",
        "        <img src=\"https://www.wordstream.com/wp-content/uploads/2021/07/how-to-get-amazon-reviews.png\" width=\"200\"/>\n",
        "    </td>\n",
        "    <td style=\"text-align: left; vertical-align: top;\">\n",
        "        <h1><strong>Amazon Reviews</strong><br></h1>\n",
        "        <h4>Engineering Large Scale Data Analytics Systems<br>\n",
        "        ENSF 612 - Fall 2023</h4>\n",
        "    </td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "PPqcEas0zbok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Note: run all the code the first time. For subsecuent runs, set run_everything flag to False. This will avoid resetting spark, mounting the drive and compute high intensive functions that were already computed.\n"
      ],
      "metadata": {
        "id": "Vj1DYh7gVroz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_everything = True"
      ],
      "metadata": {
        "id": "Dtm4ra-vWu6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Spark & Spark NLP**"
      ],
      "metadata": {
        "id": "f9WO6Sbd1kLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the capture magic command captures the output of the block to avoid clutter\n",
        "%%capture\n",
        "\n",
        "if run_everything:\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !wget https://dlcdn.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
        "  !tar -xvf spark-3.3.3-bin-hadoop3.tgz\n",
        "  !pip install findspark\n",
        "  !pip install -q spark-nlp\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "  import findspark\n",
        "  findspark.init()\n",
        "  findspark.find()\n",
        "  from pyspark.sql import SparkSession\n",
        "  import sparknlp\n",
        "\n",
        "  # Setting up 4 threads, potentially allowing a 4-core processor execute 4 tasks in parallel\n",
        "  # And adding the Spark NLP package to the Spark session\n",
        "  spark = SparkSession.builder\\\n",
        "      .appName(\"Colab\")\\\n",
        "      .master(\"local[4]\")\\\n",
        "      .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\")\\\n",
        "      .getOrCreate()\n",
        "\n",
        "  sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "Pxq1F4TUz2w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Drive & Loading Datasets**"
      ],
      "metadata": {
        "id": "MsH2kePPYp5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_everything:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PavmD6VdOyGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/Big\\ Data/datasets"
      ],
      "metadata": {
        "id": "N2zZsE4mWIEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = 'drive/MyDrive/Big Data/datasets'\n",
        "\n",
        "# Gets the list of files in the dataset directory that end in \".json\"\n",
        "json_files = [file for file in os.listdir(dataset_directory) if file.endswith('.json')]\n",
        "\n",
        "# Creates a list of full file paths\n",
        "file_paths = [os.path.join(dataset_directory, file) for file in json_files]"
      ],
      "metadata": {
        "id": "1Ezm-o9Qc6Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to parse NDJSON (new line-delimited JSON) files and extract specific fields\n",
        "def parse_ndjson(line):\n",
        "    try:\n",
        "        # Parse the JSON line and return only reviewText asin and reviewerID\n",
        "        json_line = json.loads(line)\n",
        "        return (\n",
        "            json_line.get('overall', ''),\n",
        "            json_line.get('reviewText', '')\n",
        "        )\n",
        "    except json.JSONDecodeError:\n",
        "        # In case of error, skip this record and return None\n",
        "        return None"
      ],
      "metadata": {
        "id": "Xg7bWZmDeCWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty RDD\n",
        "data_rdd = spark.sparkContext.emptyRDD()\n",
        "\n",
        "# Read each file into an RDD, parse its ndjson objects if not None, and union with the existing RDD\n",
        "for file_path in file_paths:\n",
        "    file_rdd = sc.textFile(file_path, 4)\n",
        "    parsed_rdd = file_rdd.map(parse_ndjson).filter(lambda x: x is not None)\n",
        "    data_rdd = data_rdd.union(parsed_rdd)\n",
        "\n",
        "# convert the data_rdd to a distributed Spark DataFrame\n",
        "df = spark.createDataFrame(data_rdd, schema=['score', 'review'])"
      ],
      "metadata": {
        "id": "sZauPMutEHHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Inspection**"
      ],
      "metadata": {
        "id": "leZYCxGgri_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show # Shows the attributes of the DataFrame"
      ],
      "metadata": {
        "id": "DAJd5ZPvge4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count() # Amount of records on the DataFrame"
      ],
      "metadata": {
        "id": "hXIqIZ070agO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1) # Preview a single record"
      ],
      "metadata": {
        "id": "1JB8WoYvVfgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_everything:\n",
        "  from pyspark.sql.functions import col, count\n",
        "\n",
        "  # Check if there are missing values in the dataset (If necessary, we would fill-in missing values with an appropiate method)\n",
        "  for column in df.columns:\n",
        "      null_count = df.filter(col(column).isNull()).count()\n",
        "      print(f\"Number of nulls in column {column}: {null_count}\")"
      ],
      "metadata": {
        "id": "RR6lncBXVMo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Pre-Processing**"
      ],
      "metadata": {
        "id": "c_1op0sWyQwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expanding contractions\n",
        "\n",
        "Although this step in not estrictly neccesary. Expanding contractions can make the text clearer and more consistent for the model, which can improve its ability to interpret and analyze the words."
      ],
      "metadata": {
        "id": "XDaErNxGPY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "import contractions\n",
        "\n",
        "# Define the UDF for expanding contractions\n",
        "def expand_contractions_text(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "expand_contractions_udf = udf(expand_contractions_text, StringType())\n",
        "\n",
        "# Apply the UDF to the DataFrame to create a new column with expanded contractions\n",
        "expanded_df = df.withColumn(\"expanded_review\", expand_contractions_udf(\"review\"))"
      ],
      "metadata": {
        "id": "k4-9n1ILO_YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining DocumentAssembler and Spark NLP components\n",
        "\n",
        "The DocumentAssembler is the initial step in a Spark NLP pipeline. It converts raw text into a structured Annotation format that subsequent Spark NLP annotators can utilize for processing."
      ],
      "metadata": {
        "id": "Ng2X3BH7L_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"expanded_review\") \\\n",
        "    .setOutputCol(\"document\")"
      ],
      "metadata": {
        "id": "xeNjrdmjLzDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization"
      ],
      "metadata": {
        "id": "ASy-qlIWyfeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "j1KTfat5MLHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Text Cleaning"
      ],
      "metadata": {
        "id": "cfFm5Gm3zPLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\") \\\n",
        "    .setLowercase(True) \\\n",
        "    .setCleanupPatterns([\"[^A-Za-z'\\\\s]\"])  # remove punctuations and numbers"
      ],
      "metadata": {
        "id": "gJjSrwUCMPLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stopword Removal"
      ],
      "metadata": {
        "id": "3skTysCEz_RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"cleanTokens\")"
      ],
      "metadata": {
        "id": "bjNBhlUVMS8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Stemming/Lemmatization.\n",
        "\n",
        "Stemming and lemmatization are both text normalization techniques that reduce words to their base or root form. Applying both can at times be redundant. For this application we decide to use Lemmatization.\n"
      ],
      "metadata": {
        "id": "A8p8I4tF0Ib1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pretrained LemmatizerModel from Spark NLP\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"lemmatized\")"
      ],
      "metadata": {
        "id": "KnrQsjcEMdPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Spark NLP pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    stop_words_cleaner,\n",
        "    lemmatizer\n",
        "])\n",
        "\n",
        "processed = pipeline.fit(expanded_df).transform(expanded_df)\n",
        "\n",
        "# Show the processed data\n",
        "processed.show()"
      ],
      "metadata": {
        "id": "Z31hZhtxEOO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first row of the DataFrame\n",
        "first_row = processed.first()\n",
        "\n",
        "# Print first row with its content\n",
        "print(\"Score:\", first_row['score'])\n",
        "print(\"Review:\", first_row['review'])\n",
        "print(\"Document:\", [doc.result for doc in first_row['document']])\n",
        "print(\"Token:\", [tok.result for tok in first_row['token']])\n",
        "print(\"Normalized:\", [norm.result for norm in first_row['normalized']])\n",
        "print(\"Clean Tokens:\", [clean.result for clean in first_row['cleanTokens']])\n",
        "print(\"Lemmatized:\", [lemma.result for lemma in first_row['lemmatized']])\n"
      ],
      "metadata": {
        "id": "1PW-gHXqJ0t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature extraction**"
      ],
      "metadata": {
        "id": "yoZ0k4Olwv79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps:\n",
        "\n",
        "Feature extraction (Bag of Words, TF-IDF, word embeddings Word2Vec)\n",
        "\n",
        "Vectorization (Count Vectorizer, TfidfVectorizer)\n",
        "\n",
        "Model selection (LogisticRegression, Nayve Bayes, SVM or unsupervised learning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Additional steps (optional to improve accuracy):\n",
        "\n",
        "Speech tagging (before stop word removal)\n",
        "\n",
        "N-grams to use along with TD-IDF"
      ],
      "metadata": {
        "id": "GgxZ4b794R6g"
      }
    }
  ]
}