{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MENG2023-TP/ENSF612-Project/blob/main/Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "    <td>\n",
        "        <img src=\"https://www.wordstream.com/wp-content/uploads/2021/07/how-to-get-amazon-reviews.png\" width=\"200\"/>\n",
        "    </td>\n",
        "    <td style=\"text-align: left; vertical-align: top;\">\n",
        "        <h1><strong>Amazon Reviews</strong><br></h1>\n",
        "        <h4>Engineering Large Scale Data Analytics Systems<br>\n",
        "        ENSF 612 - Fall 2023</h4>\n",
        "    </td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "PPqcEas0zbok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Note: run all the code the first time. For subsecuent runs, you can set the individual process flags below to False. This will avoid resetting spark, reloading the datasets or repeat computing intensive tasks that were previously computed and stored.\n"
      ],
      "metadata": {
        "id": "Vj1DYh7gVroz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_spark = True\n",
        "load_datasets = True\n",
        "inspect_data = True\n",
        "pre_process = True"
      ],
      "metadata": {
        "id": "Dtm4ra-vWu6K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Spark, Spark NLP and Required Modules**"
      ],
      "metadata": {
        "id": "f9WO6Sbd1kLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the capture magic command captures the output of the block to avoid clutter\n",
        "%%capture\n",
        "\n",
        "if set_spark:\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !wget https://dlcdn.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
        "  !tar -xvf spark-3.3.3-bin-hadoop3.tgz\n",
        "  !pip install findspark\n",
        "  !pip install -q spark-nlp\n",
        "  !pip install contractions\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "  import findspark\n",
        "  findspark.init()\n",
        "  findspark.find()\n",
        "  from pyspark.sql import SparkSession\n",
        "  import sparknlp\n",
        "\n",
        "  # Setting up 4 threads, potentially allowing a 4-core processor execute 4 tasks in parallel\n",
        "  # And adding the Spark NLP package to the Spark session\n",
        "  spark = SparkSession.builder\\\n",
        "      .appName(\"Colab\")\\\n",
        "      .master(\"local[4]\")\\\n",
        "      .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\")\\\n",
        "      .getOrCreate()\n",
        "\n",
        "  sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "Pxq1F4TUz2w8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning Github Repository and Loading Datasets**"
      ],
      "metadata": {
        "id": "MsH2kePPYp5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if load_datasets:\n",
        "  !git clone https://github.com/MENG2023-TP/ENSF612-Project.git"
      ],
      "metadata": {
        "id": "GmhKWpfEiyK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d0e565-daf1-43b6-de1c-b63ddf1850a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ENSF612-Project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ENSF612-Project/datasets"
      ],
      "metadata": {
        "id": "N2zZsE4mWIEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045b83ad-99b3-4386-825a-2808384c65cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All_Beauty_5.json  Cell_Phones_and_Accessories_5_subsample.json  Software_5.json\n",
            "Appliances_5.json  Musical_Instruments_5_subsample.json\t\t Video_Games_5_subsample.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = 'ENSF612-Project/datasets'\n",
        "\n",
        "# Gets the list of files in the dataset directory that end in \".json\"\n",
        "json_files = [file for file in os.listdir(dataset_directory) if file.endswith('.json')]\n",
        "\n",
        "# Creates a list of full file paths\n",
        "file_paths = [os.path.join(dataset_directory, file) for file in json_files]"
      ],
      "metadata": {
        "id": "1Ezm-o9Qc6Ii"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to parse NDJSON (new line-delimited JSON) files and extract specific fields\n",
        "def parse_ndjson(line):\n",
        "    try:\n",
        "        # Parse the JSON line and return only reviewText asin and reviewerID\n",
        "        json_line = json.loads(line)\n",
        "        return (\n",
        "            json_line.get('overall', ''),\n",
        "            json_line.get('reviewText', '')\n",
        "        )\n",
        "    except json.JSONDecodeError:\n",
        "        # In case of error, skip this record and return None\n",
        "        return None"
      ],
      "metadata": {
        "id": "Xg7bWZmDeCWJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if load_datasets:\n",
        "  # Initialize an empty RDD\n",
        "  data_rdd = spark.sparkContext.emptyRDD()\n",
        "\n",
        "  # Read each file into an RDD, parse its ndjson objects if not None, and union with the existing RDD\n",
        "  for file_path in file_paths:\n",
        "      file_rdd = sc.textFile(file_path, 4)\n",
        "      parsed_rdd = file_rdd.map(parse_ndjson).filter(lambda x: x is not None)\n",
        "      data_rdd = data_rdd.union(parsed_rdd)\n",
        "\n",
        "  # convert the data_rdd to a distributed Spark DataFrame\n",
        "  df = spark.createDataFrame(data_rdd, schema=['score', 'review']).cache()"
      ],
      "metadata": {
        "id": "sZauPMutEHHW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Inspection**"
      ],
      "metadata": {
        "id": "leZYCxGgri_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(df) # Shows type of df"
      ],
      "metadata": {
        "id": "DAJd5ZPvge4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc39195-6e8b-4e34-8b4d-31cb34cc56e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df # Shows attributes of df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1qYQz9JsRcf",
        "outputId": "f8316cc2-897f-472c-bdf3-d82705c05f10"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[score: double, review: string]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inspect_data:\n",
        "  count = df.count() # Count amount of records in the DataFrame\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXIqIZ070agO",
        "outputId": "f851c193-c586-451a-d0b4-2e972d62e003"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118242"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(1) # Preview a single record"
      ],
      "metadata": {
        "id": "1JB8WoYvVfgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfd8a34-457a-4675-8dc1-4c3871301a1e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(score=5.0, review='I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inspect_data:\n",
        "  from pyspark.sql.functions import col, sum\n",
        "  null_count = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).cache()  # Count null values"
      ],
      "metadata": {
        "id": "RR6lncBXVMo9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_count.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJEF_T7Dq2qc",
        "outputId": "1cb5a4ad-0167-49ea-91cb-fadd3cd083b8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|score|review|\n",
            "+-----+------+\n",
            "|    0|     0|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Pre-Processing**"
      ],
      "metadata": {
        "id": "c_1op0sWyQwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expanding contractions\n",
        "\n",
        "Although this step in not estrictly neccesary. Expanding contractions can make the text clearer and more consistent for the model, which can improve its ability to interpret and analyze the words."
      ],
      "metadata": {
        "id": "XDaErNxGPY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if pre_process:\n",
        "  from pyspark.sql.functions import udf\n",
        "  from pyspark.sql.types import StringType\n",
        "  import contractions\n",
        "\n",
        "  # Define the UDF for expanding contractions\n",
        "  def expand_contractions_text(text):\n",
        "      return contractions.fix(text)\n",
        "\n",
        "  expand_contractions_udf = udf(expand_contractions_text, StringType())\n",
        "\n",
        "  # Apply the UDF to the DataFrame to create a new column with expanded contractions\n",
        "  expanded_df = df.withColumn(\"expanded_review\", expand_contractions_udf(\"review\"))"
      ],
      "metadata": {
        "id": "k4-9n1ILO_YA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining DocumentAssembler and Spark NLP components\n",
        "\n",
        "The DocumentAssembler is the initial step in a Spark NLP pipeline. It converts raw text into a structured Annotation format that subsequent Spark NLP annotators can utilize for processing."
      ],
      "metadata": {
        "id": "Ng2X3BH7L_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"expanded_review\") \\\n",
        "    .setOutputCol(\"document\")"
      ],
      "metadata": {
        "id": "xeNjrdmjLzDm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization"
      ],
      "metadata": {
        "id": "ASy-qlIWyfeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "j1KTfat5MLHe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Text Cleaning"
      ],
      "metadata": {
        "id": "cfFm5Gm3zPLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\") \\\n",
        "    .setLowercase(True) \\\n",
        "    .setCleanupPatterns([\"[^A-Za-z'\\\\s]\"])  # remove punctuations and numbers"
      ],
      "metadata": {
        "id": "gJjSrwUCMPLE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stopword Removal"
      ],
      "metadata": {
        "id": "3skTysCEz_RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"cleanTokens\")"
      ],
      "metadata": {
        "id": "bjNBhlUVMS8t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Stemming/Lemmatization.\n",
        "\n",
        "Stemming and lemmatization are both text normalization techniques that reduce words to their base or root form. Applying both can at times be redundant. For this application we decide to use Lemmatization.\n"
      ],
      "metadata": {
        "id": "A8p8I4tF0Ib1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pretrained LemmatizerModel from Spark NLP\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"lemmatized\")"
      ],
      "metadata": {
        "id": "KnrQsjcEMdPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af915198-9429-40ef-c45a-0c608774a96e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if pre_process:\n",
        "  # Define the Spark NLP pipeline\n",
        "  from pyspark.ml import Pipeline\n",
        "\n",
        "  preprocessing = Pipeline(stages=[\n",
        "      document_assembler,\n",
        "      tokenizer,\n",
        "      normalizer,\n",
        "      stop_words_cleaner,\n",
        "      lemmatizer\n",
        "  ])\n",
        "\n",
        "  processed = preprocessing.fit(expanded_df).transform(expanded_df).cache()"
      ],
      "metadata": {
        "id": "Z31hZhtxEOO8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the processed data\n",
        "processed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7IiBlvAwu5T",
        "outputId": "8b2831a3-65cf-49a0-8bcf-61f4cfecbc9b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|score|              review|     expanded_review|            document|               token|          normalized|         cleanTokens|          lemmatized|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  5.0|I like this as a ...|I like this as a ...|[{document, 0, 15...|[{token, 0, 0, I,...|[{token, 0, 0, i,...|[{token, 2, 5, li...|[{token, 2, 5, li...|\n",
            "|  5.0|           good item|           good item|[{document, 0, 8,...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|\n",
            "|  5.0|Fit my new LG dry...|Fit my new LG dry...|[{document, 0, 29...|[{token, 0, 2, Fi...|[{token, 0, 2, fi...|[{token, 0, 2, fi...|[{token, 0, 2, fi...|\n",
            "|  5.0|Good value for el...|Good value for el...|[{document, 0, 29...|[{token, 0, 3, Go...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|\n",
            "|  5.0|Price and deliver...|Price and deliver...|[{document, 0, 32...|[{token, 0, 4, Pr...|[{token, 0, 4, pr...|[{token, 0, 4, pr...|[{token, 0, 4, pr...|\n",
            "|  5.0|I purchasaed a ne...|I purchasaed a ne...|[{document, 0, 17...|[{token, 0, 0, I,...|[{token, 0, 0, i,...|[{token, 2, 11, p...|[{token, 2, 11, p...|\n",
            "|  4.0|         Good value.|         Good value.|[{document, 0, 10...|[{token, 0, 3, Go...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|\n",
            "|  2.0|works great. we l...|works great. we l...|[{document, 0, 10...|[{token, 0, 4, wo...|[{token, 0, 4, wo...|[{token, 0, 4, wo...|[{token, 0, 4, wo...|\n",
            "|  2.0|Luved it for the ...|Luved it for the ...|[{document, 0, 41...|[{token, 0, 4, Lu...|[{token, 0, 4, lu...|[{token, 0, 4, lu...|[{token, 0, 4, lu...|\n",
            "|  1.0|Be careful, NewAi...|Be careful, NewAi...|[{document, 0, 39...|[{token, 0, 1, Be...|[{token, 0, 1, be...|[{token, 3, 9, ca...|[{token, 3, 9, ca...|\n",
            "|  1.0|We would give les...|We would give les...|[{document, 0, 58...|[{token, 0, 1, We...|[{token, 0, 1, we...|[{token, 9, 12, g...|[{token, 9, 12, g...|\n",
            "|  1.0|Be careful, NewAi...|Be careful, NewAi...|[{document, 0, 39...|[{token, 0, 1, Be...|[{token, 0, 1, be...|[{token, 3, 9, ca...|[{token, 3, 9, ca...|\n",
            "|  1.0|We would give les...|We would give les...|[{document, 0, 58...|[{token, 0, 1, We...|[{token, 0, 1, we...|[{token, 9, 12, g...|[{token, 9, 12, g...|\n",
            "|  1.0|We would give les...|We would give les...|[{document, 0, 58...|[{token, 0, 1, We...|[{token, 0, 1, we...|[{token, 9, 12, g...|[{token, 9, 12, g...|\n",
            "|  5.0|       Great product|       Great product|[{document, 0, 12...|[{token, 0, 4, Gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|\n",
            "|  5.0|Did the job for f...|Did the job for f...|[{document, 0, 38...|[{token, 0, 2, Di...|[{token, 0, 2, di...|[{token, 8, 10, j...|[{token, 8, 10, j...|\n",
            "|  2.0|               cheap|               cheap|[{document, 0, 4,...|[{token, 0, 4, ch...|[{token, 0, 4, ch...|[{token, 0, 4, ch...|[{token, 0, 4, ch...|\n",
            "|  5.0|Great deal and fa...|Great deal and fa...|[{document, 0, 28...|[{token, 0, 4, Gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|\n",
            "|  5.0|All was fine, pro...|All was fine, pro...|[{document, 0, 39...|[{token, 0, 2, Al...|[{token, 0, 2, al...|[{token, 8, 11, f...|[{token, 8, 11, f...|\n",
            "|  5.0|         received ok|         received ok|[{document, 0, 10...|[{token, 0, 7, re...|[{token, 0, 7, re...|[{token, 0, 7, re...|[{token, 0, 7, re...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first row of the DataFrame\n",
        "first_row = processed.first()\n",
        "\n",
        "# Print first row with its content\n",
        "print(\"Score:\", first_row['score'])\n",
        "print(\"Review:\", first_row['review'])\n",
        "print(\"Document:\", [doc.result for doc in first_row['document']])\n",
        "print(\"Token:\", [tok.result for tok in first_row['token']])\n",
        "print(\"Normalized:\", [norm.result for norm in first_row['normalized']])\n",
        "print(\"Clean Tokens:\", [clean.result for clean in first_row['cleanTokens']])\n",
        "print(\"Lemmatized:\", [lemma.result for lemma in first_row['lemmatized']])"
      ],
      "metadata": {
        "id": "1PW-gHXqJ0t5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2024d207-600c-4841-b782-f21fbadcb113"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 5.0\n",
            "Review: I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.\n",
            "Document: ['I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.']\n",
            "Token: ['I', 'like', 'this', 'as', 'a', 'vent', 'as', 'well', 'as', 'something', 'that', 'will', 'keep', 'house', 'warmer', 'in', 'winter', '.', 'I', 'sanded', 'it', 'and', 'then', 'painted', 'it', 'the', 'same', 'color', 'as', 'the', 'house', '.', 'Looks', 'great', '.']\n",
            "Normalized: ['i', 'like', 'this', 'as', 'a', 'vent', 'as', 'well', 'as', 'something', 'that', 'will', 'keep', 'house', 'warmer', 'in', 'winter', 'i', 'sanded', 'it', 'and', 'then', 'painted', 'it', 'the', 'same', 'color', 'as', 'the', 'house', 'looks', 'great']\n",
            "Clean Tokens: ['like', 'vent', 'well', 'something', 'keep', 'house', 'warmer', 'winter', 'sanded', 'painted', 'color', 'house', 'looks', 'great']\n",
            "Lemmatized: ['like', 'vent', 'well', 'something', 'keep', 'house', 'warm', 'winter', 'sand', 'paint', 'color', 'house', 'look', 'great']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature engineering**"
      ],
      "metadata": {
        "id": "yoZ0k4Olwv79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "wHIgXtpx10fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "tfidf_vectorizer = IDF() \\\n",
        "    .setInputCol(\"lemmatized\") \\\n",
        "    .setOutputCol(\"tfidf_features\")"
      ],
      "metadata": {
        "id": "6l3qrOMt11nS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Machine Learning Models**"
      ],
      "metadata": {
        "id": "vuGdTEVu41d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model Selection"
      ],
      "metadata": {
        "id": "T9k619pt55OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for supervised learning classifiers\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "juOHzf1S40Rq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating Models**"
      ],
      "metadata": {
        "id": "9C02hm6Z5IWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QNcN992T4z9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps:\n",
        "\n",
        "Feature extraction (Bag of Words, TF-IDF, word embeddings Word2Vec)\n",
        "\n",
        "Vectorization (Count Vectorizer, TfidfVectorizer)\n",
        "\n",
        "Model selection (LogisticRegression, Nayve Bayes, SVM or unsupervised learning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Additional steps (optional to improve accuracy):\n",
        "\n",
        "Speech tagging (before stop word removal)\n",
        "\n",
        "N-grams to use along with TD-IDF"
      ],
      "metadata": {
        "id": "GgxZ4b794R6g"
      }
    }
  ]
}